{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# 0. Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import pickle as pkl\n",
    "import cv2\n",
    "import albumentations\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "from PIL import Image\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    print('Invalid device or cannot modify virtual devices once initialized.')\n",
    "\n",
    "from tensorflow.keras import models, layers, regularizers, metrics, losses, optimizers, constraints\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# https://iupac.org/100/stories/what-on-earth-is-inchi/\n",
    "# tokenizer -> https://www.kaggle.com/yasufuminakama/inchi-preprocess-2\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project='Bristol', entity='enric1296')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Imgs train: 2424186\n",
      "Num Imgs test: 1616107\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b414dacfbdc84df1abae120701937e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2424186.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99150d40a3da4c52b5d40efcfbb4c2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1616107.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd03250862cd4493b9e4e9762eb86730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2424186.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8984af4591824e15ae40b7f325096aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1616107.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "# 1. Paths\n",
    "\n",
    "PATH_DATA = '../01_Data/'\n",
    "PATH_MODELS = '../03_Models/'\n",
    "\n",
    "PATH_DATA_TRAIN = PATH_DATA + 'train/'\n",
    "PATH_DATA_TEST = PATH_DATA + 'test/'\n",
    "\n",
    "list_imgs_train_paths = glob.glob(PATH_DATA_TRAIN + '/*/*/*/*')\n",
    "list_imgs_test_paths = glob.glob(PATH_DATA_TEST + '/*/*/*/*')\n",
    "\n",
    "print(f'Num Imgs train: {len(list_imgs_train_paths)}')\n",
    "print(f'Num Imgs test: {len(list_imgs_test_paths)}')\n",
    "\n",
    "list_imgs_train = [path.split('\\\\')[-1].split('.')[0] for path in tqdm(list_imgs_train_paths)]\n",
    "list_imgs_test = [path.split('\\\\')[-1].split('.')[0] for path in tqdm(list_imgs_test_paths)]\n",
    "\n",
    "dict_imgs_train_paths = {path.split('\\\\')[-1].split('.')[0] : path for path in tqdm(list_imgs_train_paths)}\n",
    "dict_imgs_test_paths = {path.split('\\\\')[-1].split('.')[0] : path for path in tqdm(list_imgs_test_paths)}\n",
    "\n",
    "\n",
    "VERSION = '001'\n",
    "SEQ_LEN_INCHI = 275\n",
    "SEQ_LEN_REST_INCHI = 255\n",
    "SEQ_LEN_CHFORM = 20\n",
    "IMG_SIZE = (220, 340)\n",
    "\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# 2. Classes & Functions\n",
    "\n",
    "def splitFormula(form):\n",
    "    string = ''\n",
    "    for i in re.findall(r\"[A-Z][^A-Z]*\", form):\n",
    "        elem = re.match(r\"\\D+\", i).group()\n",
    "        num = i.replace(elem, \"\")\n",
    "        if num == \"\":\n",
    "            string += f\"{elem} \"\n",
    "        else:\n",
    "            string += f\"{elem} {str(num)} \"\n",
    "    return string.rstrip(' ')\n",
    "\n",
    "\n",
    "def splitFormula2(form):\n",
    "    string = ''\n",
    "    for i in re.findall(r\"[a-z][^a-z]*\", form):\n",
    "        elem = i[0]\n",
    "        num = i.replace(elem, \"\").replace('/', \"\")\n",
    "        num_string = ''\n",
    "        for j in re.findall(r\"[0-9]+[^0-9]*\", num):\n",
    "            num_list = list(re.findall(r'\\d+', j))\n",
    "            assert len(num_list) == 1, f\"len(num_list) != 1\"\n",
    "            _num = num_list[0]\n",
    "            if j == _num:\n",
    "                num_string += f\"{_num} \"\n",
    "            else:\n",
    "                extra = j.replace(_num, \"\")\n",
    "                num_string += f\"{_num} {' '.join(list(extra))} \"\n",
    "        string += f\"/{elem} {num_string}\"\n",
    "    return string.rstrip(' ')\n",
    "\n",
    "\n",
    "class Tokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "    \n",
    "    def fit_on_texts(self, texts):\n",
    "        vocab = set(['<pad>', '<sos>', '<eos>'])\n",
    "        for text in texts:\n",
    "            vocab.update(text.split(' '))\n",
    "        for i, s in enumerate(vocab):\n",
    "            self.stoi[s] = i\n",
    "        self.itos = {item[1]: item[0] for item in self.stoi.items()}\n",
    "        \n",
    "    def text_to_sequence(self, text):\n",
    "        sequence = []\n",
    "        sequence.append(self.stoi['<sos>'])\n",
    "        for s in text.split(' '):\n",
    "            sequence.append(self.stoi[s])\n",
    "        sequence.append(self.stoi['<eos>'])\n",
    "        return sequence\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            sequence = self.text_to_sequence(text)\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "\n",
    "    def sequence_to_text(self, sequence):\n",
    "        return ''.join(list(map(lambda i: self.itos[i], sequence)))\n",
    "    \n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            text = self.sequence_to_text(sequence)\n",
    "            texts.append(text)\n",
    "        return texts\n",
    "    \n",
    "    def predict_caption(self, sequence):\n",
    "        caption = ''\n",
    "        for i in sequence:\n",
    "            if i == self.stoi['<eos>'] or i == self.stoi['<pad>']:\n",
    "                break\n",
    "            caption += self.itos[i]\n",
    "        return caption\n",
    "    \n",
    "    def predict_captions(self, sequences):\n",
    "        captions = []\n",
    "        for sequence in sequences:\n",
    "            caption = self.predict_caption(sequence)\n",
    "            captions.append(caption)\n",
    "        return captions\n",
    "\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# 3. Load Data\n",
    "\n",
    "# 2.0 - Load labels\n",
    "\n",
    "with open(f'{PATH_MODELS}df_train_labels_{VERSION}.pkl', 'rb') as f:\n",
    "    df_train_labels = pkl.load(f)\n",
    "\n",
    "# 2.1 - Load tokenizer\n",
    "\n",
    "with open(f'{PATH_MODELS}tokenizer_experiment_{VERSION}.pkl', 'rb') as f:\n",
    "    tokenizer = pkl.load(f)\n",
    "    \n",
    "EMB_DIM = len(tokenizer.stoi)\n",
    "\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531de29115624e3f85d2cdc163509a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2424186.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83288b798994fb5a7bb13e8379cb9af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2424186.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<sos>C10H18N2O9P2/c1-7-10(13)9(5-11-2-3-20-22(14,15)16)8(4-12-7)6-21-23(17,18)19/h4,11,13H,2-3,5-6H2,1H3,(H2,14,15,16)(H2,17,18,19)<eos>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7272586"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################################\n",
    "# 4. Preprocess  \n",
    "\n",
    "df_train_labels['ch_form'] = df_train_labels['l_0'].apply(lambda x: splitFormula(x))\n",
    "df_train_labels['rest_inchi'] = df_train_labels.apply(lambda x: x['text'][len(x ['ch_form']):].strip(), axis=1)\n",
    "\n",
    "dict_train_data = {}\n",
    "list_rows = df_train_labels.to_dict('records')\n",
    "for row in tqdm(list_rows):\n",
    "    image_id = row['image_id']\n",
    "    # ch_form = tokenizer.texts_to_sequences([row['ch_form']])\n",
    "    #rest_inchi = tokenizer.texts_to_sequences([row['rest_inchi']])\n",
    "    complete_inchi = tokenizer.texts_to_sequences([row['text']])\n",
    "    #dict_train_data[image_id] = {'ch_form' : ch_form, 'rest_inchi' : rest_inchi, 'complete_inchi' : complete_inchi}\n",
    "    dict_train_data[image_id] = {'complete_inchi' : complete_inchi}\n",
    "\n",
    "list_paths, list_labels, list_target_input = [], [], []\n",
    "for img_id in tqdm(dict_train_data):\n",
    "    path = [dict_imgs_train_paths[img_id]]\n",
    "    label = dict_train_data[img_id]['complete_inchi']\n",
    "    # tar_inp = [dict_train_data[img_id]['ch_form'][0][:-1] + dict_train_data[img_id]['rest_inchi'][0][1:]]\n",
    "    label = pad_sequences(label, maxlen=SEQ_LEN_INCHI+1, padding='post', value=tokenizer.stoi['<pad>'])\n",
    "    #tar_inp = pad_sequences(tar_inp, maxlen=SEQ_LEN_INCHI+1, padding='post', value=tokenizer.stoi['<pad>'])\n",
    "    list_paths.extend(path)\n",
    "    list_labels.extend(label)\n",
    "    #list_target_input.extend(tar_inp)\n",
    "\n",
    "print(''.join([tokenizer.itos[s] for s in dict_train_data[image_id]['complete_inchi'][0]]))\n",
    "\n",
    "del df_train_labels, list_rows, dict_train_data\n",
    "gc.collect()\n",
    "\n",
    "# print([tokenizer.itos[s] for s in dict_train_data[image_id]['ch_form'][0]])\n",
    "# print([tokenizer.itos[s] for s in dict_train_data[image_id]['rest_inchi'][0]])\n",
    "# print([tokenizer.itos[s] for s in dict_train_data[image_id]['complete_inchi'][0]])\n",
    "\n",
    "# print([tokenizer.itos[s] for s in list_labels[0]][:60])\n",
    "\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainTransformations():\n",
    "    return albumentations.Compose([\n",
    "        albumentations.Flip(p=0.3)\n",
    "    ])\n",
    "\n",
    "def getAngles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positionalEncoding(position, trf_dim):\n",
    "    angle_rads = getAngles(np.arange(position)[:, np.newaxis],\n",
    "                           np.arange(trf_dim)[np.newaxis, :],\n",
    "                           trf_dim)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float16)\n",
    "    \n",
    "    \n",
    "def buildImgLoader(with_labels=True, target_size=(300, 300), ext='png'):\n",
    "    def decode(path):\n",
    "        file_bytes = tf.io.read_file(path)\n",
    "        if ext == 'png':\n",
    "            img = tf.image.decode_png(file_bytes, channels=3)\n",
    "        elif ext in ['jpg', 'jpeg']:\n",
    "            img = tf.image.decode_jpeg(file_bytes, channels=3)\n",
    "        else:\n",
    "            raise ValueError(\"Image extension not supported\")\n",
    "\n",
    "        img = tf.cast(img, tf.float32)# / 255.0\n",
    "        img = tf.image.resize(img, target_size)\n",
    "        return img\n",
    "    \n",
    "    def decode_with_labels(path, label):\n",
    "        img = decode(path)\n",
    "        return img, label#tf.data.Dataset.from_tensors((img, label))\n",
    "    \n",
    "    return decode_with_labels\n",
    "\n",
    "\n",
    "def buildTransformations(list_transforms):\n",
    "    def process_data(image, label):\n",
    "        image = tf.image.random_flip_up_down(image)\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        return image, label\n",
    "    return process_data\n",
    "\n",
    "\n",
    "def createPaddingMask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.bool)\n",
    "    # add extra dimensions to add the padding to the attention logits.\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    mask = mask[:, tf.newaxis, tf.newaxis, :]\n",
    "    return mask\n",
    "\n",
    "def causal_attention_mask(seq_len):\n",
    "    i = tf.range(seq_len)[:, tf.newaxis]\n",
    "    j = tf.range(seq_len)\n",
    "    mask = tf.cast(i >= j, dtype=tf.int32)\n",
    "    mask = tf.reshape(mask, (-1, seq_len, seq_len))\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(1, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "        axis=0,\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "def buildTarInput():\n",
    "    def buildTarInput_(img_, label_):\n",
    "        tar_input = label_[:, :-1]\n",
    "        label_ = label_[:, 1:]\n",
    "        return img_, tar_input, label_\n",
    "    return buildTarInput_\n",
    "\n",
    "\n",
    "def buildMasks(batch_size):\n",
    "    def createMasks(img_, tar_inp_, label_):\n",
    "        batch_size = label_.shape[0]\n",
    "        look_ahead_mask = causal_attention_mask(SEQ_LEN_INCHI)  \n",
    "        look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "        pad_mask = createPaddingMask(tar_inp_)\n",
    "        return (img_, tar_inp_, look_ahead_mask, pad_mask), label_\n",
    "    \n",
    "    return createMasks\n",
    "\n",
    "\n",
    "def build_dataset(paths, labels=None, bsize=32, list_transforms=True, shuffle=True):\n",
    "\n",
    "    img_loader = buildImgLoader(with_labels=True, target_size=(IMG_SIZE[0], IMG_SIZE[1]))\n",
    "    if list_transforms:\n",
    "        img_augmenter = buildTransformations(list_transforms)\n",
    "    tar_input_creator = buildTarInput()\n",
    "    mask_creator = buildMasks(batch_size=bsize)\n",
    "    \n",
    "    AUTO = tf.data.AUTOTUNE\n",
    "    slices = (paths, labels)\n",
    "    \n",
    "    dset = tf.data.Dataset.from_tensor_slices(slices).prefetch(AUTO)\n",
    "    if shuffle:\n",
    "        dset = dset.shuffle(len(labels))\n",
    "    dset = dset.map(img_loader, num_parallel_calls=AUTO)\n",
    "    if list_transforms:\n",
    "        dset = dset.map(img_augmenter, num_parallel_calls=AUTO)\n",
    "    dset = dset.batch(bsize, drop_remainder=True).prefetch(AUTO)\n",
    "    dset = dset.map(tar_input_creator, num_parallel_calls=AUTO)\n",
    "    dset = dset.map(mask_creator, num_parallel_calls=AUTO)  \n",
    "    return dset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 220, 340, 3) (8, 275) (1, 275, 275) (8, 1, 1, 275) (8, 275)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'C', '29', 'H', '37', 'N', '5', 'O', '2', 'S', '/c', '1', '-', '6', '-', '34', '-', '24', '-', '12', '-', '11', '-', '22', '(', '17', '-', '25', '(', '24', ')', '32', '(', '5', ')', '27', '(', '35', ')', '29', '(', '3', ',', '4', ')', '28', '(', '34', ')', '36', ')', '10', '-', '8', '-', '15', '-', '33', '(', '19', '-', '26', '-', '31', '-', '21', '(', '2', ')', '20', '-', '37', '-', '26', ')', '16', '-', '13', '-', '23', '-', '9', '-', '7', '-', '14', '-', '30', '-', '18', '-', '23', '/h', '7', ',', '9', ',', '11', '-', '12']\n",
      "['C', '29', 'H', '37', 'N', '5', 'O', '2', 'S', '/c', '1', '-', '6', '-', '34', '-', '24', '-', '12', '-', '11', '-', '22', '(', '17', '-', '25', '(', '24', ')', '32', '(', '5', ')', '27', '(', '35', ')', '29', '(', '3', ',', '4', ')', '28', '(', '34', ')', '36', ')', '10', '-', '8', '-', '15', '-', '33', '(', '19', '-', '26', '-', '31', '-', '21', '(', '2', ')', '20', '-', '37', '-', '26', ')', '16', '-', '13', '-', '23', '-', '9', '-', '7', '-', '14', '-', '30', '-', '18', '-', '23', '/h', '7', ',', '9', ',', '11', '-', '12', ',']\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(list_imgs_train)-16)\n",
    "\n",
    "dtrain = build_dataset(list_paths[idx:idx+16], list_labels[idx:idx+16],\n",
    "                       list_transforms=getTrainTransformations(), bsize=8, \n",
    "                       shuffle=True)\n",
    "\n",
    "for batch in dtrain:\n",
    "    break\n",
    "\n",
    "data, target = batch\n",
    "\n",
    "imgs, target_input, look_ahead_mask, pad_mask = data\n",
    "print(imgs.shape, target_input.shape, look_ahead_mask.shape, pad_mask.shape, target.shape) \n",
    "\n",
    "plt.imshow(imgs[0]/255., cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print([tokenizer.itos[s] for s in target_input[0].numpy()[:100]])\n",
    "print([tokenizer.itos[s] for s in target[0].numpy()[:100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(5):\n",
    "#     print(f'Epoch: {epoch}')\n",
    "#     for batch in dtrain:\n",
    "#         data, target = batch\n",
    "#         imgs, target_input, look_ahead_mask, pad_mask = data\n",
    "#         print(imgs.shape)\n",
    "#         plt.imshow(imgs[0]/255., cmap='gray')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0 - max -> 26 it/s, mean -> 22 it/s\n",
    "# v1 - max -> 80 it/s, mean -> 74 it/s\n",
    "# dtrain = build_dataset(list_paths[:10_000], list_labels[:10_000], list_transforms=getTrainTransformations(), bsize=8)\n",
    "# for i, batch in tqdm(enumerate(dtrain)):\n",
    "#     if i==10_000:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "\n",
    "class EncoderImgs(models.Model):\n",
    "    def __init__(self, model_base_path, trf_dim):\n",
    "        super(EncoderImgs, self).__init__()\n",
    "        self.trf_dim = trf_dim\n",
    "        self.backbone_model = tf.keras.applications.InceptionV3(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
    "                                                                include_top=False, weights=None)\n",
    "        self.backbone_model.trainable = True\n",
    "        self.base_model = tf.keras.Model(self.backbone_model.input, \n",
    "                                         self.backbone_model.layers[-1].output)\n",
    "        self.drop1 = layers.Dropout(0.2)\n",
    "        self.cnn_final = layers.Conv2D(SEQ_LEN_INCHI, 1, padding='same', activation='relu')\n",
    "        self.reshape = layers.Reshape((5*9, SEQ_LEN_INCHI), input_shape=(5, 9, SEQ_LEN_INCHI))#7*11\n",
    "        self.linear_drop = layers.Dropout(0.2)\n",
    "        self.linear_proj = models.Sequential([\n",
    "            layers.Dense(trf_dim, activation='linear'),\n",
    "            layers.LayerNormalization(epsilon=1e-6)\n",
    "        ])\n",
    "\n",
    "        \n",
    "    def call(self, img_input, training):\n",
    "        x = img_input\n",
    "        x = tf.keras.applications.inception_v3.preprocess_input(x)\n",
    "        x = self.base_model(x, training=training)\n",
    "        x = self.drop1(x, training=training)\n",
    "        x = self.cnn_final(x)\n",
    "        # print(x.shape)\n",
    "        ##\n",
    "        # x = tf.reshape(x, [tf.shape(x)[0], tf.shape(x)[1]*tf.shape(x)[2], tf.shape(x)[3]])\n",
    "        x = self.reshape(x)\n",
    "        # print(x.shape)\n",
    "        x = tf.transpose(x, perm=[0, 2, 1])\n",
    "        x = self.linear_drop(x, training)\n",
    "        x = self.linear_proj(x)\n",
    "        ##\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class EncoderTransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, attention_axes=None, rate=0.1):\n",
    "        super(EncoderTransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, attention_axes=attention_axes)\n",
    "        self.ffn = models.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), \n",
    "             layers.Dense(embed_dim)]\n",
    "        )\n",
    "        \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, query, key, training, attention_mask=None):\n",
    "        attn_output = self.att(query, key, attention_mask=attention_mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        \n",
    "        out1 = self.layernorm1(query + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        return self.layernorm2(out1 + ffn_output)    \n",
    "    \n",
    "\n",
    "class DecoderTransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, attention_axes=None, rate=0.1):\n",
    "        super(DecoderTransformerBlock, self).__init__()\n",
    "        self.att_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, attention_axes=attention_axes)\n",
    "        self.att_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, attention_axes=attention_axes)\n",
    "        self.ffn = models.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), \n",
    "             layers.Dense(embed_dim)], name='ffn_dec'\n",
    "        )\n",
    "        \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, query, key, enc_output, training=True, pad_mask=None, look_ahead_mask=None):\n",
    "        att1 = self.att_1(query, key, attention_mask=look_ahead_mask)\n",
    "        att1 = self.dropout1(att1, training=training)\n",
    "        out1 = self.layernorm1(query + att1)\n",
    "        \n",
    "        att2 = self.att_2(out1, enc_output, attention_mask=pad_mask)\n",
    "        att2 = self.dropout2(att2, training=training)\n",
    "        out2 = self.layernorm2(out1 + att2)\n",
    "        \n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        \n",
    "        return self.layernorm3(out2 + ffn_output)\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, trf_dim, num_heads, ff_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.trf_dim = trf_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = layers.Embedding(EMB_DIM, trf_dim)\n",
    "        self.dropout = layers.Dropout(0.1)\n",
    "        self.list_transformer_blocks = [EncoderTransformerBlock(trf_dim, num_heads, ff_dim, rate=0.1)\n",
    "                                           for _ in range(num_layers)]\n",
    "        \n",
    "    def call(self, enc_img_input, training=True):        \n",
    "        x = self.dropout(enc_img_input, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.list_transformer_blocks[i](x, x, training=training, attention_mask=None)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, trf_dim, num_heads, ff_dim, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.trf_dim = trf_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = layers.Embedding(EMB_DIM, trf_dim)\n",
    "        self.dropout = layers.Dropout(0.1)\n",
    "        self.pos_encoding = positionalEncoding(SEQ_LEN_INCHI, trf_dim)\n",
    "        self.list_transformer_blocks = [DecoderTransformerBlock(trf_dim, num_heads, ff_dim, rate=0.1)\n",
    "                                           for _ in range(num_layers)]\n",
    "        \n",
    "    def call(self, inputs, training=True):\n",
    "        enc_out, dec_input, look_ahead_mask, pad_mask = inputs\n",
    "        seq_len = tf.shape(dec_input)[1]\n",
    "        \n",
    "        x = self.embedding(dec_input)\n",
    "        x *= tf.math.sqrt(tf.cast(self.trf_dim, tf.float16))#tf.float16))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.list_transformer_blocks[i](x, x, enc_out, training=training, \n",
    "                                                look_ahead_mask=look_ahead_mask, pad_mask=None)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class TransformerModel(models.Model):\n",
    "    def __init__(self, model_base_path, trf_dim, num_heads, ff_dim, num_enc_layers, num_dec_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.trf_dim = trf_dim\n",
    "        self.img_encoder = EncoderImgs(model_base_path, trf_dim)\n",
    "        self.encoder = Encoder(trf_dim=trf_dim, num_heads=num_heads, ff_dim=trf_dim*4, num_layers=num_enc_layers)\n",
    "        self.decoder = Decoder(trf_dim=trf_dim, num_heads=num_heads, ff_dim=trf_dim*4, num_layers=num_dec_layers)\n",
    "        ###\n",
    "        self.final_layer = layers.Dense(EMB_DIM, activation='linear')\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        ###\n",
    "        imgs, in_target, look_ahead_mask, pad_mask = inputs\n",
    "        seq_len = tf.shape(in_target)[1]\n",
    "        ### \n",
    "        ###\n",
    "        imgs_features = self.img_encoder(imgs, training)\n",
    "        encoder_out = self.encoder(imgs_features, training)\n",
    "        x = self.decoder((encoder_out, in_target, look_ahead_mask, pad_mask), training=training)\n",
    "        logits = self.final_layer(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    \n",
    "def getTransformerModel(model_img_base_path, trf_dim, num_heads, ff_dim, num_enc_layers, num_dec_layers):\n",
    "    return TransformerModel(model_img_base_path, trf_dim, num_heads, ff_dim, num_enc_layers, num_dec_layers)\n",
    "\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.not_equal(real, 0)\n",
    "    loss_ = loss_object(tf.cast(real, tf.float32), tf.cast(pred, tf.float32))\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(tf.cast(real, tf.float32), tf.cast(tf.argmax(pred, axis=2), tf.float32))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float16)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.cast(tf.math.minimum(arg1, arg2), tf.float16)#, tf.float32)\n",
    "    \n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, IMG_SIZE[0], IMG_SIZE[1], 3), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(None, SEQ_LEN_INCHI), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(None, SEQ_LEN_INCHI, SEQ_LEN_INCHI), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(None, 1, 1, SEQ_LEN_INCHI), dtype=tf.bool),\n",
    "    tf.TensorSpec(shape=(None, SEQ_LEN_INCHI), dtype=tf.float32)\n",
    "]   \n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(imgs, target_input, look_ahead_mask, pad_mask, tar_real):\n",
    "    with tf.GradientTape() as tape:\n",
    "        inputs = (imgs, target_input, look_ahead_mask, pad_mask)\n",
    "        predictions = transformer(inputs, training=True)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "        scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "        \n",
    "    scaled_gradients = tape.gradient(scaled_loss, transformer.trainable_variables)\n",
    "    gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "#     gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))  \n",
    "    \n",
    "    \n",
    "test_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, IMG_SIZE[0], IMG_SIZE[1], 3), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(None, SEQ_LEN_INCHI), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(None, SEQ_LEN_INCHI, SEQ_LEN_INCHI), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(None, 1, 1, SEQ_LEN_INCHI), dtype=tf.bool),\n",
    "    tf.TensorSpec(shape=(None, SEQ_LEN_INCHI), dtype=tf.float32)\n",
    "]   \n",
    "@tf.function(input_signature=test_step_signature)\n",
    "def test_step(imgs, target_input, look_ahead_mask, pad_mask, tar_real):\n",
    "    inputs = (imgs, target_input, look_ahead_mask, pad_mask)\n",
    "    predictions = transformer(inputs, training=False)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "    val_loss(loss)\n",
    "    val_accuracy(accuracy_function(tar_real, predictions)) \n",
    "    pred_tokens = tf.argmax(predictions, -1)\n",
    "    val_metric(get_levenshtein_distance(tf.cast(pred_tokens, tf.float64), tf.cast(tar_real, tf.float64)))\n",
    "    \n",
    "def dense_to_sparse(dense):\n",
    "    indices = tf.where(tf.ones_like(dense))\n",
    "    values = tf.reshape(dense, (SEQ_LEN_INCHI*batch_size,))\n",
    "    sparse = tf.SparseTensor(indices, values, (batch_size, SEQ_LEN_INCHI))\n",
    "    return sparse\n",
    "\n",
    "def get_levenshtein_distance(preds, lbls):\n",
    "    bs = preds.shape[0]\n",
    "    preds = tf.where(tf.not_equal(lbls, tokenizer.stoi['<eos>']) & tf.not_equal(lbls, tokenizer.stoi['<pad>']), preds, 0)\n",
    "    lbls = tf.where(tf.not_equal(lbls, tokenizer.stoi['<eos>']), lbls, 0)\n",
    "\n",
    "    preds_sparse = dense_to_sparse(preds)\n",
    "    lbls_sparse = dense_to_sparse(lbls)\n",
    "\n",
    "    batch_distance = tf.edit_distance(preds_sparse, lbls_sparse, normalize=False)\n",
    "    mean_distance = tf.math.reduce_mean(batch_distance)\n",
    "    \n",
    "    return mean_distance\n",
    "\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 2060, compute capability 7.5\n",
      "Latest checkpoint restored!!\n",
      "================================================================================\n",
      "Epoch 1\n",
      "Train Epoch 1 Time 61.67 Batch 0/14250 Loss 1.5904 Accuracy 0.7460\n",
      "Train Epoch 1 Time 122.24 Batch 250/14250 Loss 0.1005 Accuracy 0.9679\n",
      "Train Epoch 1 Time 182.56 Batch 500/14250 Loss 0.0877 Accuracy 0.9707\n",
      "Train Epoch 1 Time 241.54 Batch 750/14250 Loss 0.0831 Accuracy 0.9718\n",
      "Train Epoch 1 Time 300.03 Batch 1000/14250 Loss 0.0808 Accuracy 0.9723\n",
      "Train Epoch 1 Time 358.69 Batch 1250/14250 Loss 0.0787 Accuracy 0.9728\n",
      "Train Epoch 1 Time 417.22 Batch 1500/14250 Loss 0.0770 Accuracy 0.9733\n",
      "Train Epoch 1 Time 476.21 Batch 1750/14250 Loss 0.0761 Accuracy 0.9736\n",
      "Train Epoch 1 Time 534.98 Batch 2000/14250 Loss 0.0752 Accuracy 0.9738\n",
      "Train Epoch 1 Time 593.31 Batch 2250/14250 Loss 0.0745 Accuracy 0.9740\n",
      "Train Epoch 1 Time 651.92 Batch 2500/14250 Loss 0.0738 Accuracy 0.9742\n",
      "Train Epoch 1 Time 710.22 Batch 2750/14250 Loss 0.0736 Accuracy 0.9742\n",
      "Train Epoch 1 Time 768.73 Batch 3000/14250 Loss 0.0734 Accuracy 0.9743\n",
      "Train Epoch 1 Time 827.30 Batch 3250/14250 Loss 0.0730 Accuracy 0.9744\n",
      "Train Epoch 1 Time 885.80 Batch 3500/14250 Loss 0.0732 Accuracy 0.9744\n",
      "Train Epoch 1 Time 944.19 Batch 3750/14250 Loss 0.0730 Accuracy 0.9744\n",
      "Train Epoch 1 Time 1002.73 Batch 4000/14250 Loss 0.0727 Accuracy 0.9745\n",
      "Train Epoch 1 Time 1061.50 Batch 4250/14250 Loss 0.0724 Accuracy 0.9746\n",
      "Train Epoch 1 Time 1119.96 Batch 4500/14250 Loss 0.0722 Accuracy 0.9746\n",
      "Train Epoch 1 Time 1178.46 Batch 4750/14250 Loss 0.0720 Accuracy 0.9747\n",
      "Train Epoch 1 Time 1237.17 Batch 5000/14250 Loss 0.0718 Accuracy 0.9748\n",
      "Train Epoch 1 Time 1295.75 Batch 5250/14250 Loss 0.0716 Accuracy 0.9748\n",
      "Train Epoch 1 Time 1354.18 Batch 5500/14250 Loss 0.0715 Accuracy 0.9749\n",
      "Train Epoch 1 Time 1412.52 Batch 5750/14250 Loss 0.0714 Accuracy 0.9749\n",
      "Train Epoch 1 Time 1471.07 Batch 6000/14250 Loss 0.0713 Accuracy 0.9749\n",
      "Train Epoch 1 Time 1530.75 Batch 6250/14250 Loss 0.0712 Accuracy 0.9750\n",
      "Train Epoch 1 Time 1590.06 Batch 6500/14250 Loss 0.0710 Accuracy 0.9750\n",
      "Train Epoch 1 Time 1648.60 Batch 6750/14250 Loss 0.0710 Accuracy 0.9750\n",
      "Train Epoch 1 Time 1706.96 Batch 7000/14250 Loss 0.0710 Accuracy 0.9750\n",
      "Train Epoch 1 Time 1765.39 Batch 7250/14250 Loss 0.0708 Accuracy 0.9751\n",
      "Train Epoch 1 Time 1823.66 Batch 7500/14250 Loss 0.0707 Accuracy 0.9751\n",
      "Train Epoch 1 Time 1882.22 Batch 7750/14250 Loss 0.0707 Accuracy 0.9751\n",
      "Train Epoch 1 Time 1940.75 Batch 8000/14250 Loss 0.0706 Accuracy 0.9752\n",
      "Train Epoch 1 Time 1999.20 Batch 8250/14250 Loss 0.0704 Accuracy 0.9752\n",
      "Train Epoch 1 Time 2057.72 Batch 8500/14250 Loss 0.0704 Accuracy 0.9752\n",
      "Train Epoch 1 Time 2116.29 Batch 8750/14250 Loss 0.0704 Accuracy 0.9752\n",
      "Train Epoch 1 Time 2174.78 Batch 9000/14250 Loss 0.0703 Accuracy 0.9752\n",
      "Train Epoch 1 Time 2233.33 Batch 9250/14250 Loss 0.0703 Accuracy 0.9753\n",
      "Train Epoch 1 Time 2291.95 Batch 9500/14250 Loss 0.0702 Accuracy 0.9753\n",
      "Train Epoch 1 Time 2350.37 Batch 9750/14250 Loss 0.0701 Accuracy 0.9753\n",
      "Train Epoch 1 Time 2408.98 Batch 10000/14250 Loss 0.0701 Accuracy 0.9753\n",
      "Train Epoch 1 Time 2467.22 Batch 10250/14250 Loss 0.0700 Accuracy 0.9753\n",
      "Train Epoch 1 Time 2525.67 Batch 10500/14250 Loss 0.0700 Accuracy 0.9753\n",
      "Train Epoch 1 Time 2584.34 Batch 10750/14250 Loss 0.0699 Accuracy 0.9754\n",
      "Train Epoch 1 Time 2642.82 Batch 11000/14250 Loss 0.0698 Accuracy 0.9754\n",
      "Train Epoch 1 Time 2701.39 Batch 11250/14250 Loss 0.0698 Accuracy 0.9754\n",
      "Train Epoch 1 Time 2760.30 Batch 11500/14250 Loss 0.0697 Accuracy 0.9754\n",
      "Train Epoch 1 Time 2819.83 Batch 11750/14250 Loss 0.0697 Accuracy 0.9754\n",
      "Train Epoch 1 Time 2879.08 Batch 12000/14250 Loss 0.0696 Accuracy 0.9754\n",
      "Train Epoch 1 Time 2938.66 Batch 12250/14250 Loss 0.0696 Accuracy 0.9754\n",
      "Train Epoch 1 Time 2997.61 Batch 12500/14250 Loss 0.0696 Accuracy 0.9754\n",
      "Train Epoch 1 Time 3056.08 Batch 12750/14250 Loss 0.0695 Accuracy 0.9755\n",
      "Train Epoch 1 Time 3114.41 Batch 13000/14250 Loss 0.0695 Accuracy 0.9755\n",
      "Train Epoch 1 Time 3173.00 Batch 13250/14250 Loss 0.0694 Accuracy 0.9755\n",
      "Train Epoch 1 Time 3231.43 Batch 13500/14250 Loss 0.0694 Accuracy 0.9755\n",
      "Train Epoch 1 Time 3289.94 Batch 13750/14250 Loss 0.0693 Accuracy 0.9755\n",
      "Train Epoch 1 Time 3348.37 Batch 14000/14250 Loss 0.0693 Accuracy 0.9755\n",
      "Val Epoch 1 Time 3413.12 Batch 0/750 Loss 2.2439 Accuracy 0.4983 Val LVSD 43.2500\n",
      "Val Epoch 1 Time 3420.47 Batch 100/750 Loss 2.3222 Accuracy 0.4912 Val LVSD 45.9431\n",
      "Val Epoch 1 Time 3427.85 Batch 200/750 Loss 2.3129 Accuracy 0.4933 Val LVSD 45.6251\n",
      "Val Epoch 1 Time 3435.34 Batch 300/750 Loss 2.3055 Accuracy 0.4947 Val LVSD 45.6389\n",
      "Val Epoch 1 Time 3442.84 Batch 400/750 Loss 2.3072 Accuracy 0.4942 Val LVSD 45.6654\n",
      "Val Epoch 1 Time 3450.30 Batch 500/750 Loss 2.3095 Accuracy 0.4937 Val LVSD 45.6720\n",
      "Val Epoch 1 Time 3457.75 Batch 600/750 Loss 2.3110 Accuracy 0.4933 Val LVSD 45.7314\n",
      "Val Epoch 1 Time 3465.18 Batch 700/750 Loss 2.3104 Accuracy 0.4934 Val LVSD 45.7243\n",
      "Saving checkpoint for epoch 1 at ../03_Models/00_Checkpoints/full_inchi_chform_opt_v1.4/\n",
      "\n",
      "Time taken for epoch 1/10: 3470.50 secs\n",
      "Epoch 1 - Train Loss 0.0693 - Train Accuracy 0.9755 - Val Loss 2.3102 - Val Accuracy 0.4935 - Val LVSD 45.7229\n",
      "================================================================================\n",
      "Epoch 2\n",
      "Train Epoch 2 Time 1.01 Batch 0/14250 Loss 0.0463 Accuracy 0.9829\n",
      "Train Epoch 2 Time 58.53 Batch 250/14250 Loss 0.0662 Accuracy 0.9763\n",
      "Train Epoch 2 Time 116.18 Batch 500/14250 Loss 0.0658 Accuracy 0.9765\n",
      "Train Epoch 2 Time 173.81 Batch 750/14250 Loss 0.0656 Accuracy 0.9765\n",
      "Train Epoch 2 Time 231.43 Batch 1000/14250 Loss 0.0655 Accuracy 0.9766\n",
      "Train Epoch 2 Time 289.09 Batch 1250/14250 Loss 0.0652 Accuracy 0.9767\n",
      "Train Epoch 2 Time 346.68 Batch 1500/14250 Loss 0.0652 Accuracy 0.9767\n",
      "Train Epoch 2 Time 404.26 Batch 1750/14250 Loss 0.0649 Accuracy 0.9768\n",
      "Train Epoch 2 Time 461.92 Batch 2000/14250 Loss 0.0647 Accuracy 0.9769\n",
      "Train Epoch 2 Time 519.58 Batch 2250/14250 Loss 0.0650 Accuracy 0.9768\n",
      "Train Epoch 2 Time 577.22 Batch 2500/14250 Loss 0.0647 Accuracy 0.9769\n",
      "Train Epoch 2 Time 634.80 Batch 2750/14250 Loss 0.0648 Accuracy 0.9769\n",
      "Train Epoch 2 Time 692.34 Batch 3000/14250 Loss 0.0649 Accuracy 0.9769\n",
      "Train Epoch 2 Time 749.93 Batch 3250/14250 Loss 0.0649 Accuracy 0.9769\n",
      "Train Epoch 2 Time 807.46 Batch 3500/14250 Loss 0.0648 Accuracy 0.9769\n",
      "Train Epoch 2 Time 865.02 Batch 3750/14250 Loss 0.0648 Accuracy 0.9769\n",
      "Train Epoch 2 Time 922.63 Batch 4000/14250 Loss 0.0650 Accuracy 0.9768\n",
      "Train Epoch 2 Time 980.20 Batch 4250/14250 Loss 0.0650 Accuracy 0.9768\n",
      "Train Epoch 2 Time 1037.81 Batch 4500/14250 Loss 0.0649 Accuracy 0.9769\n",
      "Train Epoch 2 Time 1095.39 Batch 4750/14250 Loss 0.0649 Accuracy 0.9769\n",
      "Train Epoch 2 Time 1153.52 Batch 5000/14250 Loss 0.0649 Accuracy 0.9769\n",
      "Train Epoch 2 Time 1211.19 Batch 5250/14250 Loss 0.0648 Accuracy 0.9769\n",
      "Train Epoch 2 Time 1268.90 Batch 5500/14250 Loss 0.0647 Accuracy 0.9769\n",
      "Train Epoch 2 Time 1326.61 Batch 5750/14250 Loss 0.0647 Accuracy 0.9770\n",
      "Train Epoch 2 Time 1384.97 Batch 6000/14250 Loss 0.0646 Accuracy 0.9770\n",
      "Train Epoch 2 Time 1442.62 Batch 6250/14250 Loss 0.0646 Accuracy 0.9770\n",
      "Train Epoch 2 Time 1500.52 Batch 6500/14250 Loss 0.0647 Accuracy 0.9769\n",
      "Train Epoch 2 Time 1558.24 Batch 6750/14250 Loss 0.0647 Accuracy 0.9770\n",
      "Train Epoch 2 Time 1615.95 Batch 7000/14250 Loss 0.0647 Accuracy 0.9770\n",
      "Train Epoch 2 Time 1673.65 Batch 7250/14250 Loss 0.0647 Accuracy 0.9770\n",
      "Train Epoch 2 Time 1731.32 Batch 7500/14250 Loss 0.0647 Accuracy 0.9769\n",
      "Train Epoch 2 Time 1789.46 Batch 7750/14250 Loss 0.0647 Accuracy 0.9770\n",
      "Train Epoch 2 Time 1847.12 Batch 8000/14250 Loss 0.0646 Accuracy 0.9770\n",
      "Train Epoch 2 Time 1904.77 Batch 8250/14250 Loss 0.0646 Accuracy 0.9770\n",
      "Train Epoch 2 Time 1962.45 Batch 8500/14250 Loss 0.0646 Accuracy 0.9770\n",
      "Train Epoch 2 Time 2020.11 Batch 8750/14250 Loss 0.0646 Accuracy 0.9770\n",
      "Train Epoch 2 Time 2077.75 Batch 9000/14250 Loss 0.0645 Accuracy 0.9770\n",
      "Train Epoch 2 Time 2135.38 Batch 9250/14250 Loss 0.0645 Accuracy 0.9770\n",
      "Train Epoch 2 Time 2193.06 Batch 9500/14250 Loss 0.0645 Accuracy 0.9770\n",
      "Train Epoch 2 Time 2250.70 Batch 9750/14250 Loss 0.0645 Accuracy 0.9770\n",
      "Train Epoch 2 Time 2308.35 Batch 10000/14250 Loss 0.0645 Accuracy 0.9770\n",
      "Train Epoch 2 Time 2366.07 Batch 10250/14250 Loss 0.0645 Accuracy 0.9770\n",
      "Train Epoch 2 Time 2424.16 Batch 10500/14250 Loss 0.0645 Accuracy 0.9770\n",
      "Train Epoch 2 Time 2483.63 Batch 10750/14250 Loss 0.0645 Accuracy 0.9770\n",
      "Train Epoch 2 Time 2541.92 Batch 11000/14250 Loss 0.0644 Accuracy 0.9771\n",
      "Train Epoch 2 Time 2600.09 Batch 11250/14250 Loss 0.0644 Accuracy 0.9771\n",
      "Train Epoch 2 Time 2658.17 Batch 11500/14250 Loss 0.0644 Accuracy 0.9771\n",
      "Train Epoch 2 Time 2716.48 Batch 11750/14250 Loss 0.0644 Accuracy 0.9771\n",
      "Train Epoch 2 Time 2775.09 Batch 12000/14250 Loss 0.0644 Accuracy 0.9771\n",
      "Train Epoch 2 Time 2833.33 Batch 12250/14250 Loss 0.0644 Accuracy 0.9771\n",
      "Train Epoch 2 Time 2892.19 Batch 12500/14250 Loss 0.0644 Accuracy 0.9771\n",
      "Train Epoch 2 Time 2950.55 Batch 12750/14250 Loss 0.0644 Accuracy 0.9771\n",
      "Train Epoch 2 Time 3008.22 Batch 13000/14250 Loss 0.0644 Accuracy 0.9771\n",
      "Train Epoch 2 Time 3066.40 Batch 13250/14250 Loss 0.0645 Accuracy 0.9770\n",
      "Train Epoch 2 Time 3124.62 Batch 13500/14250 Loss 0.0645 Accuracy 0.9770\n",
      "Train Epoch 2 Time 3182.46 Batch 13750/14250 Loss 0.0645 Accuracy 0.9770\n",
      "Train Epoch 2 Time 3240.67 Batch 14000/14250 Loss 0.0645 Accuracy 0.9770\n",
      "Val Epoch 2 Time 3298.77 Batch 0/750 Loss 0.0714 Accuracy 0.9741 Val LVSD 2.1500\n",
      "Val Epoch 2 Time 3305.33 Batch 100/750 Loss 0.0472 Accuracy 0.9829 Val LVSD 1.5574\n",
      "Val Epoch 2 Time 3311.95 Batch 200/750 Loss 0.0491 Accuracy 0.9825 Val LVSD 1.5998\n",
      "Val Epoch 2 Time 3318.55 Batch 300/750 Loss 0.0496 Accuracy 0.9824 Val LVSD 1.6096\n",
      "Val Epoch 2 Time 3325.07 Batch 400/750 Loss 0.0494 Accuracy 0.9824 Val LVSD 1.6030\n",
      "Val Epoch 2 Time 3331.76 Batch 500/750 Loss 0.0500 Accuracy 0.9823 Val LVSD 1.6226\n",
      "Val Epoch 2 Time 3338.55 Batch 600/750 Loss 0.0499 Accuracy 0.9822 Val LVSD 1.6274\n",
      "Val Epoch 2 Time 3345.24 Batch 700/750 Loss 0.0501 Accuracy 0.9822 Val LVSD 1.6262\n",
      "Saving checkpoint for epoch 2 at ../03_Models/00_Checkpoints/full_inchi_chform_opt_v1.4/\n",
      "\n",
      "Time taken for epoch 2/10: 3350.42 secs\n",
      "Epoch 2 - Train Loss 0.0645 - Train Accuracy 0.9770 - Val Loss 0.0500 - Val Accuracy 0.9822 - Val LVSD 1.6249\n",
      "================================================================================\n",
      "Epoch 3\n",
      "Train Epoch 3 Time 0.97 Batch 0/14250 Loss 0.0503 Accuracy 0.9837\n",
      "Train Epoch 3 Time 59.59 Batch 250/14250 Loss 0.0599 Accuracy 0.9783\n",
      "Train Epoch 3 Time 117.49 Batch 500/14250 Loss 0.0603 Accuracy 0.9782\n",
      "Train Epoch 3 Time 177.01 Batch 750/14250 Loss 0.0600 Accuracy 0.9784\n",
      "Train Epoch 3 Time 235.96 Batch 1000/14250 Loss 0.0604 Accuracy 0.9782\n",
      "Train Epoch 3 Time 294.55 Batch 1250/14250 Loss 0.0608 Accuracy 0.9781\n",
      "Train Epoch 3 Time 353.11 Batch 1500/14250 Loss 0.0613 Accuracy 0.9780\n",
      "Train Epoch 3 Time 411.99 Batch 1750/14250 Loss 0.0615 Accuracy 0.9780\n",
      "Train Epoch 3 Time 470.79 Batch 2000/14250 Loss 0.0615 Accuracy 0.9780\n",
      "Train Epoch 3 Time 529.19 Batch 2250/14250 Loss 0.0615 Accuracy 0.9780\n",
      "Train Epoch 3 Time 587.47 Batch 2500/14250 Loss 0.0617 Accuracy 0.9779\n",
      "Train Epoch 3 Time 645.39 Batch 2750/14250 Loss 0.0617 Accuracy 0.9779\n",
      "Train Epoch 3 Time 703.18 Batch 3000/14250 Loss 0.0616 Accuracy 0.9780\n",
      "Train Epoch 3 Time 761.40 Batch 3250/14250 Loss 0.0615 Accuracy 0.9780\n",
      "Train Epoch 3 Time 819.43 Batch 3500/14250 Loss 0.0617 Accuracy 0.9779\n",
      "Train Epoch 3 Time 877.29 Batch 3750/14250 Loss 0.0618 Accuracy 0.9779\n",
      "Train Epoch 3 Time 936.36 Batch 4000/14250 Loss 0.0620 Accuracy 0.9778\n",
      "Train Epoch 3 Time 996.45 Batch 4250/14250 Loss 0.0621 Accuracy 0.9778\n",
      "Train Epoch 3 Time 1055.89 Batch 4500/14250 Loss 0.0620 Accuracy 0.9778\n",
      "Train Epoch 3 Time 1113.72 Batch 4750/14250 Loss 0.0620 Accuracy 0.9778\n",
      "Train Epoch 3 Time 1171.57 Batch 5000/14250 Loss 0.0620 Accuracy 0.9778\n",
      "Train Epoch 3 Time 1229.43 Batch 5250/14250 Loss 0.0623 Accuracy 0.9777\n",
      "Train Epoch 3 Time 1287.26 Batch 5500/14250 Loss 0.0623 Accuracy 0.9778\n",
      "Train Epoch 3 Time 1345.07 Batch 5750/14250 Loss 0.0623 Accuracy 0.9778\n",
      "Train Epoch 3 Time 1403.44 Batch 6000/14250 Loss 0.0622 Accuracy 0.9778\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b9748417c47f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mtarget_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlook_ahead_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_num_printer_train\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3024\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1961\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "# Training\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.config.optimizer.set_jit(True)\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "num_epochs = 10\n",
    "trf_dim = 256\n",
    "num_enc_layers = 2\n",
    "num_dec_layers = 4\n",
    "num_heads = 4\n",
    "batch_size = 20\n",
    "val_size = 0.05\n",
    "batch_num_printer_train, batch_num_printer_val = 250, 100\n",
    "model_name = 'full_inchi_chform_opt_v1.4'\n",
    "checkpoint_path = f'../03_Models/00_Checkpoints/{model_name}/'\n",
    "restore_last_chekpoint = (True, '../03_Models/00_Checkpoints/final_model_v1.3/ckpt-7')\n",
    "\n",
    "rand_idxs = np.arange(300_000)\n",
    "np.random.shuffle(rand_idxs)\n",
    "list_paths_sample, list_labels_sample = list(np.asarray(list_paths)[rand_idxs]), list(np.asarray(list_labels)[rand_idxs])\n",
    "thresh = int(len(list_paths_sample)*(1-val_size))\n",
    "\n",
    "train_data_generator = build_dataset(list_paths_sample[:thresh], list_labels_sample[:thresh], bsize=batch_size,\n",
    "                                    list_transforms=getTrainTransformations())\n",
    "val_data_generator = build_dataset(list_paths_sample[thresh:], list_labels_sample[thresh:], bsize=batch_size,\n",
    "                                  list_transforms=None)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.Mean(name='val_accuracy')\n",
    "val_metric = tf.keras.metrics.Mean(name='val_lvsd')\n",
    "\n",
    "learning_rate = CustomSchedule(trf_dim)\n",
    "optimizer = optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "optimizer = mixed_precision.LossScaleOptimizer(optimizer)\n",
    "\n",
    "transformer = TransformerModel(None, trf_dim, num_heads, trf_dim*4, num_enc_layers, num_dec_layers)\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "if restore_last_chekpoint[0]:\n",
    "    ckpt.restore(restore_last_chekpoint[1])\n",
    "    print('Latest checkpoint restored!!')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    print('===='*20)\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "    val_metric.reset_states()\n",
    "    for batch_num, batch_data in enumerate(train_data_generator):\n",
    "        data, target = batch_data\n",
    "        imgs, target_input, look_ahead_mask, pad_mask = data\n",
    "        target = tf.cast(target, tf.float32)\n",
    "        target_input = tf.cast(target_input, tf.float32)\n",
    "        train_step(imgs, target_input, look_ahead_mask, pad_mask, target)\n",
    "\n",
    "        if batch_num % batch_num_printer_train == 0:\n",
    "            printer = f'Train Epoch {epoch + 1} Time {time.time() - start:.2f} Batch {batch_num}/{len(train_data_generator)} '\n",
    "            printer += f'Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}'\n",
    "            print(printer)\n",
    "            \n",
    "    for val_batch_num, val_batch_data in enumerate(val_data_generator):\n",
    "        data, target = val_batch_data\n",
    "        imgs, target_input, look_ahead_mask, pad_mask = data\n",
    "        target = tf.cast(target, tf.float32)\n",
    "        target_input = tf.cast(target_input, tf.float32)\n",
    "        test_step(imgs, target_input, look_ahead_mask, pad_mask, target)\n",
    "        \n",
    "        if val_batch_num % batch_num_printer_val == 0:\n",
    "            printer = f'Val Epoch {epoch + 1} Time {time.time() - start:.2f} Batch {val_batch_num}/{len(val_data_generator)} '\n",
    "            printer += f'Loss {val_loss.result():.4f} Accuracy {val_accuracy.result():.4f} Val LVSD {val_metric.result():.4f}'\n",
    "            print(printer)\n",
    "            \n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(f'Saving checkpoint for epoch {epoch+1} at {checkpoint_path}')\n",
    "    \n",
    "    print(f'\\nTime taken for epoch {epoch+1}/{num_epochs}: {time.time() - start:.2f} secs')\n",
    "    printer = f'Epoch {epoch + 1} - Train Loss {train_loss.result():.4f} - Train Accuracy {train_accuracy.result():.4f} '\n",
    "    printer += f'- Val Loss {val_loss.result():.4f} - Val Accuracy {val_accuracy.result():.4f} - Val LVSD {val_metric.result():.4f}'\n",
    "    print(printer)\n",
    "    \n",
    "    wandb.log({'epochs': epoch,\n",
    "                'train_loss': train_loss.result(),\n",
    "                'train_acc': train_accuracy.result(), \n",
    "                'val_loss': val_loss.result(),\n",
    "                'val_acc': val_accuracy.result(),\n",
    "                'val_lvsd' : val_metric.result()})\n",
    "\n",
    "\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time taken for epoch 2/10: 2052.97 secs\n",
    "# Epoch 2 - Train Loss 0.6351 - Train Accuracy 0.7764 - Val Loss 0.6790 - Val Accuracy 0.7737 - Val LVSD 20.5271\n",
    "\n",
    "# Time taken for epoch 2/10: 2078.89 secs\n",
    "# Epoch 2 - Train Loss 0.6421 - Train Accuracy 0.7740 - Val Loss 0.5503 - Val Accuracy 0.8042 - Val LVSD 17.8266\n",
    "\n",
    "# Time taken for epoch 2/10: 2788.95 secs\n",
    "# Epoch 2 - Train Loss 0.5599 - Train Accuracy 0.8037 - Val Loss 0.4647 - Val Accuracy 0.8364 - Val LVSD 14.9557"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(50k) # Time taken for epoch 10/10: 838.28 secs\n",
    "# Epoch 10 - Train Loss 0.7291 - Train Accuracy 0.7444 - Val Loss 0.7268 - Val Accuracy 0.7479 - Val LVSD 22.8804\n",
    "# (250k) # Time taken for epoch 6/10: 4300.91 secs\n",
    "# Epoch 6 - Train Loss 0.6127 - Train Accuracy 0.7827 - Val Loss 0.5705 - Val Accuracy 0.7979 - Val LVSD 18.3881"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Inference\n",
    "\n",
    "def getInfValidTransformations():\n",
    "    return albumentations.Compose([\n",
    "        albumentations.Resize(IMG_SIZE[0], IMG_SIZE[1], always_apply=True)])\n",
    "\n",
    "def getInfFixTestTransformations():\n",
    "    return albumentations.Compose([\n",
    "        albumentations.Transpose(always_apply=True),\n",
    "        albumentations.VerticalFlip(always_apply=True),\n",
    "        albumentations.Resize(IMG_SIZE[0], IMG_SIZE[1], always_apply=True)])\n",
    "\n",
    "\n",
    "def createPaddingMask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.bool)\n",
    "    # add extra dimensions to add the padding to the attention logits.\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    mask = mask[:, tf.newaxis, :]\n",
    "    return mask\n",
    "    \n",
    "def causal_attention_mask(seq_len):\n",
    "    i = tf.range(seq_len)[:, tf.newaxis]\n",
    "    j = tf.range(seq_len)\n",
    "    mask = tf.cast(i >= j, dtype=tf.int32)\n",
    "    mask = tf.reshape(mask, (-1, seq_len, seq_len))\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(1, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "        axis=0,\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "def createMasks(tar_inp):\n",
    "    look_ahead_mask = causal_attention_mask(tf.shape(tar_inp)[1])   \n",
    "    return look_ahead_mask\n",
    "\n",
    "def readImageTF(img_path, ext='jpg'):\n",
    "    file_bytes = tf.io.read_file(img_path)\n",
    "    if ext == 'png':\n",
    "        img = tf.image.decode_png(file_bytes, channels=3)\n",
    "    elif ext in ['jpg', 'jpeg']:\n",
    "        img = tf.image.decode_jpeg(file_bytes, channels=3)\n",
    "    else:\n",
    "        raise ValueError(\"Image extension not supported\")\n",
    "\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    h_src = tf.shape(img)[0]\n",
    "    w_src = tf.shape(img)[1] \n",
    "            \n",
    "    if h_src>w_src: # pad width\n",
    "        img = tf.image.rot90(img)\n",
    "        \n",
    "    img = tf.image.resize(img, (IMG_SIZE[0], IMG_SIZE[1]))\n",
    "    return img.numpy()\n",
    "    \n",
    "    \n",
    "inference_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, SEQ_LEN_INCHI, 256), dtype=tf.float16),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None, None), dtype=tf.int32)\n",
    "]   \n",
    "@tf.function(input_signature=inference_step_signature)\n",
    "def predictToken(imgs_features, output, look_ahead_mask):\n",
    "    dec_out = model.decoder((imgs_features, output, look_ahead_mask, None), training=False)\n",
    "    predictions = model.final_layer(dec_out)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    return predicted_id    \n",
    "    \n",
    "    \n",
    "def evaluateListImg(list_img_path, model, max_length=10, trf_dim=256):\n",
    "    batch_size = tf.constant(len(list_img_path))\n",
    "    imgs = tf.concat([tf.expand_dims(readImageTF(img_path), 0) for img_path in list_img_path], axis=0)\n",
    "\n",
    "    output = tf.convert_to_tensor([tokenizer.stoi['<sos>'] for _ in range(batch_size)])\n",
    "    output = tf.expand_dims(output, -1)\n",
    "    \n",
    "    imgs_features = model.img_encoder(imgs, training=False)\n",
    "    imgs_features = model.encoder(imgs_features, training=False)\n",
    "    # Placeholder for not padding the image encoder as  we want all the seq_len from encoder\n",
    "    for _ in tf.range(max_length):\n",
    "        look_ahead_mask = createMasks(output)\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        \n",
    "        # inputs = (imgs, output, look_ahead_mask, pos_encoding)\n",
    "        # predictions = model(inputs, training=False)\n",
    "#         inputs = (imgs_features, output, look_ahead_mask, None)\n",
    "#         dec_out = model.decoder(inputs, training=False)\n",
    "#         predictions = model.final_layer(dec_out)\n",
    "        \n",
    "#         # select the last word from the seq_len dimension\n",
    "#         predictions = predictions[:, -1:, :]\n",
    "#         predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "        predicted_id = predictToken(imgs_features, output, look_ahead_mask)\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "#         # return the result if the predicted_id is equal to the end token\n",
    "        if tf.reduce_sum(tf.cast(tf.reduce_any(tf.equal(output, tokenizer.stoi['<eos>'])\\\n",
    "                                               , -1), tf.int32))== batch_size:\n",
    "            break\n",
    "                \n",
    "    return output, imgs\n",
    "\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Load pub sub and predict only not found by rdkit\n",
    "\n",
    "df_sub = pd.read_csv('../99_FinalSubmission/0_submission_2_53_norm.csv')\n",
    "df_sub['in_found_rdkit'] = df_sub['InChI'].apply(lambda x : 0 if x is np.nan else 1)\n",
    "\n",
    "dict_test_not_found = {elem['image_id'] : elem['in_found_rdkit'] \n",
    "                       for elem in df_sub[['image_id', 'in_found_rdkit']].to_dict('records')}\n",
    "\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint restored!!\n",
      "Imgs not found: 73863\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6af5b639f9346b78eac6164aa1423bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1155.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b97de2287519>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mchunk_images\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_chunks_imgs_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mlist_paths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdict_imgs_test_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimg_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimg_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchunk_images\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluateListImg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSEQ_LEN_INCHI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrf_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrf_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mpred_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mpred_inchi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'InChI=1S/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence_to_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<eos>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpred_seq\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-084ed172b922>\u001b[0m in \u001b[0;36mevaluateListImg\u001b[1;34m(list_img_path, model, max_length, trf_dim)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;31m#         predictions = predictions[:, -1:, :]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;31m#         predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mpredicted_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictToken\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlook_ahead_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3024\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1961\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "\n",
    "with open(f'./list_nf.pkl', 'rb') as f:\n",
    "    list_imgs_test_nf = pkl.load(f)\n",
    "\n",
    "# Inference\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "batch_size = 64\n",
    "trf_dim = 256\n",
    "num_enc_layers = 2\n",
    "num_dec_layers = 4\n",
    "num_heads = 4\n",
    "path_load_weights = PATH_MODELS + '00_Checkpoints/final_model_v1_4/ckpt-12'\n",
    "model = TransformerModel(None, trf_dim, num_heads, trf_dim*4, num_enc_layers, num_dec_layers)\n",
    "learning_rate = CustomSchedule(trf_dim)\n",
    "optimizer = optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "optimizer = mixed_precision.LossScaleOptimizer(optimizer)\n",
    "ckpt = tf.train.Checkpoint(transformer=model, optimizer=optimizer)\n",
    "ckpt.restore(path_load_weights)\n",
    "print('Checkpoint restored!!')\n",
    "\n",
    "# Only not found\n",
    "# list_imgs_test_nf = [img_id for img_id in dict_test_not_found if dict_test_not_found[img_id]==0]\n",
    "print(f'Imgs not found: {len(list_imgs_test_nf)}')\n",
    "# list_imgs_test_nf = list_imgs_test.copy()\n",
    "list_chunks_imgs_test = [list_imgs_test_nf[i:(i+batch_size)] for i in range(0, len(list_imgs_test_nf), batch_size)]   \n",
    "\n",
    "list_image_id, list_pred_inchis = [], []\n",
    "for chunk_images in tqdm(list_chunks_imgs_test, position=0):\n",
    "    list_paths = [dict_imgs_test_paths[img_id] for img_id in chunk_images]\n",
    "    prediction, imgs_ = evaluateListImg(list_paths, model, max_length=SEQ_LEN_INCHI, trf_dim=trf_dim)\n",
    "    pred_seq = prediction.numpy()[:, 1:]\n",
    "    pred_inchi = ['InChI=1S/' + tokenizer.sequence_to_text(seq).split('<eos>')[0] for seq in pred_seq]\n",
    "    assert len(pred_inchi)==len(list_paths)\n",
    "    list_image_id.extend(chunk_images)\n",
    "    list_pred_inchis.extend(pred_inchi)\n",
    "    \n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_bytes = tf.io.read_file(dict_imgs_test_paths[list_image_id[1337]])\n",
    "# img = tf.image.decode_png(file_bytes, channels=3)\n",
    "# img = tf.cast(img, tf.float32)\n",
    "# h_src = tf.shape(img)[0]\n",
    "# w_src = tf.shape(img)[1] \n",
    "\n",
    "# if h_src>w_src: # pad width\n",
    "#     img = tf.image.rot90(img)\n",
    "\n",
    "# img = tf.image.resize(img, (IMG_SIZE[0], IMG_SIZE[1]))\n",
    "    \n",
    "# plt.imshow(img/255.)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(imgs_[1337]/255.)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Prepare submission\n",
    "\n",
    "df_predictions = pd.DataFrame({\n",
    "    'image_id' : list_image_id,\n",
    "    'InChI' : list_pred_inchis\n",
    "})\n",
    "\n",
    "df_merged = pd.merge(df_sub, df_predictions, how='left', suffixes=('_1', '_2'), on='image_id')\n",
    "                                 \n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149c35773c544a37af861dc9de2460fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1616107.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>InChI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4c0618918583</td>\n",
       "      <td>InChI=1S/C22H20Br3N11O2/c23-8-2-10(24)17(33-16(8)34)22(38)29-5-7-1-11(32-18(35)12-4-14(26)15(25)31-12)9(7)3-28-21(27)35-20-13(6)34-21(20)30-21/h2,4,7,9,11,28H,1,3,5,26H2,(H2,27,31)(H,29,35)(H,33,34)(H3,30,32,38)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25e547a2765a</td>\n",
       "      <td>InChI=1S/C18H16O3/c1-3-10-9-14-15(17(20)11(10)4-2)18(21)13-8-6-5-7-12(13)16(14)19/h5-9,20H,3-4H2,1-2H3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4c06208f97ab</td>\n",
       "      <td>InChI=1S/C17H13ClFN5O2S/c18-12-4-13(19)11(16(26)23-9-1-2-9)3-10(12)8-5-22-24(7-8)14-6-21-17(27-14)15(20)25/h3-7,9H,1-2H2,(H2,20,25)(H,23,26)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25e54ec918f0</td>\n",
       "      <td>InChI=1S/C26H26N4O2/c1-3-30-18(2)27-23-16-21(14-15-24(23)30)26(32)29-28-25(31)17-22(19-10-6-4-7-11-19)20-12-8-5-9-13-20/h4-16,22H,3,17H2,1-2H3,(H,28,31)(H,29,32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4c06283ae4de</td>\n",
       "      <td>InChI=1S/C14H16N2O3/c1-19-7-6-16(12-3-4-12)13-5-2-10(14(17)18)8-11(13)9-15/h2,5,8,12H,3-4,6-7H2,1H3,(H,17,18)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id  \\\n",
       "0  4c0618918583   \n",
       "1  25e547a2765a   \n",
       "2  4c06208f97ab   \n",
       "3  25e54ec918f0   \n",
       "4  4c06283ae4de   \n",
       "\n",
       "                                                                                                                                                                                                                 InChI  \n",
       "0  InChI=1S/C22H20Br3N11O2/c23-8-2-10(24)17(33-16(8)34)22(38)29-5-7-1-11(32-18(35)12-4-14(26)15(25)31-12)9(7)3-28-21(27)35-20-13(6)34-21(20)30-21/h2,4,7,9,11,28H,1,3,5,26H2,(H2,27,31)(H,29,35)(H,33,34)(H3,30,32,38)  \n",
       "1                                                                                                               InChI=1S/C18H16O3/c1-3-10-9-14-15(17(20)11(10)4-2)18(21)13-8-6-5-7-12(13)16(14)19/h5-9,20H,3-4H2,1-2H3  \n",
       "2                                                                         InChI=1S/C17H13ClFN5O2S/c18-12-4-13(19)11(16(26)23-9-1-2-9)3-10(12)8-5-22-24(7-8)14-6-21-17(27-14)15(20)25/h3-7,9H,1-2H2,(H2,20,25)(H,23,26)  \n",
       "3                                                    InChI=1S/C26H26N4O2/c1-3-30-18(2)27-23-16-21(14-15-24(23)30)26(32)29-28-25(31)17-22(19-10-6-4-7-11-19)20-12-8-5-9-13-20/h4-16,22H,3,17H2,1-2H3,(H,28,31)(H,29,32)  \n",
       "4                                                                                                        InChI=1S/C14H16N2O3/c1-19-7-6-16(12-3-4-12)13-5-2-10(14(17)18)8-11(13)9-15/h2,5,8,12H,3-4,6-7H2,1H3,(H,17,18)  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################################\n",
    "# Build submission\n",
    "\n",
    "list_imgs_id = []\n",
    "list_values_inchi = []\n",
    "list_elems = df_merged.to_dict('records')\n",
    "for elem in tqdm(list_elems):\n",
    "    if elem['in_found_rdkit']==0 and elem['InChI_2'] is not np.nan:\n",
    "        if len(elem['InChI_2'])>=20:\n",
    "            inchi_ = elem['InChI_2']\n",
    "        else:\n",
    "            inchi_ = elem['InChI_1']\n",
    "    else:\n",
    "        inchi_ = elem['InChI_1'] \n",
    "    list_values_inchi.append(inchi_)  \n",
    "    list_imgs_id.append(elem['image_id'])\n",
    "\n",
    "df_submission = pd.DataFrame({\n",
    "    'image_id' : list_imgs_id,\n",
    "    'InChI' : list_values_inchi,\n",
    "})\n",
    "\n",
    "df_submission.to_csv('../04_Submissions/Iter3/submission_fix_not_found_3_15.csv', index=False)\n",
    "df_submission.head()\n",
    "\n",
    "###################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
